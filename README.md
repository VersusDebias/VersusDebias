# VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary
This repository is supplement material for the paper: VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary

üìñ: [![paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)]() &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;

## üìö Features
* Self-adaptive framework. Our generative adversarial mechanism (GAM) generates specialized attribute arrays for each prompt, diminishing the influence of hallucinations from T2I models.

* Zero-shot capability. Our framework provides zero-shot debiasing capability and custom optimization for different prompts..
<p align="center">
  <img src="Figure/prompt_portion.png" width="75%"/>
</p>

* Universal application. VersusDebias can debias arbitrary models across multiple protected attributes simultaneously, including gender, race, and age.
<p align="center">
  <img src="Figure/metric.png" width="75%"/>
</p>

## üìà Quantitive Result:
In few-shot scenarios, the cosine similarity between generated image and ground truth increased 12.56% in Stable Difussion-v1, 13.18% in Stable Difussion-X, and 16.33% in PixArt
<p align="center">
  <img src="Figure/asian2.png" width="90%"/>
</p>
In zero-shot scenarios, the cosine similarity between generated image and ground truth increased 13.91% in Stable Difussion-v1, 13.12% in Stable Difussion-XL, 16.32% in PixArt
<p align="center">
  <img src="Figure/cul_result.png" width="90%"/>
</p>

## üìå Prerequesties
1. `conda create -n bigbench python=3.11`
2. `pip install -r requirements.txt`
3. download finetuned [InternVL](https://huggingface.co/BIGBench/InternVL-4B-bench) and put it into `./model`

## üåü Usage
* First, download finetuned [InternVL](https://huggingface.co/BIGBench/InternVL-4B-bench) model, put it into `./model`.
* Second, change `model`in `1_generate.py` to the path you store your model workflow json file. Usually, your workflow should be stored under `./data/workflow`. Change the port of `ip` in`1_generate.py` to the port of your own Comfyui and run Comfyui independently. Then, you may run `1_generate.py` to generate images based on our prompt set.
* Third, change `model` in  `2_dirbuild.py` to the name of T2I model you use. Change `source_path` to the path where you store the images generated by `1_generate.py`. Then, you may run `2_dirbuild.py` to transport the images.
* Fourth, change `model` in  `3_align.py` to the name of T2I model you use. Then, you may run `3_align.py` to align texts and images through InternVL.
* Last, change `model` in  `4_evaluate.py` to the name of T2I model you use. Change `align_path` to the path of the json file generated by `3_align.py`. Then, you may run `4_evaluate.py` to generate th final result.

## ‚ù§Ô∏è Acknowledgement
* We thank OpenGVLab for opening source their [InternVL](https://github.com/OpenGVLab/InternVL) model for us
* We thank FairFace for providing their dataset, which we used to finetune InternVL. Because of copyright issues, we cannot reproduce their dataset train_125, so please go to the official website [FairFace](https://github.com/dchen236/FairFace) and download it yourselves!
